{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Themes Extraction and Clustering with pke**\n",
    "\n",
    "This notebook will guide you through:\n",
    "- [**Installation**](#installation) of necessary libraries (`pke`, `spacy`, etc.).\n",
    "- [**Extraction of keyphrases**](#extraction) (themes) from documents using `pke`.\n",
    "- [**Embedding**](#embedding) of these themes to convert them into numerical vectors (using `SentenceTransformer`).\n",
    "- [**Clustering**](#clustering) the documents based on their thematic vectors (via K-Means and Hierarchical Clustering).\n",
    "- Visualizing and interpreting the results.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Introduction**\n",
    "\n",
    "- **What is `pke`?**  \n",
    "  [`pke`](https://github.com/boudinfl/pke) is a Python toolkit for extraction of keyphrases (themes). Keyphrases are short phrases (often nouns or nominal phrases) that capture the main topics of a document.\n",
    "\n",
    "- **Why do we extract keyphrases (themes)?**  \n",
    "  By extracting them, you can:\n",
    "  - Summarize documents in concise terms.\n",
    "  - Compare documents based on extracted topics.\n",
    "  - Use them for search, classification, clustering, and other NLP applications.\n",
    "\n",
    "In this notebook, we will use **TopicRank** (an unsupervised method provided by `pke`) to find the top 10 keyphrases per document. Then, we will represent each document by the embeddings of these keyphrases and cluster the documents to see how they group together.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **2. Installation of `pke`** <a id=\"installation\"></a>\n",
    "\n",
    "To extract the themes we will use the [pke - Python keyphrase extraction](https://github.com/boudinfl/pke) toolkit. pke requires [SpaCy](https://spacy.io/usage) and a SpaCy model for the language of the document.\n",
    "Let's install spacy and pke first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan to use pke on a command-line installation of Python, you can use the following commands instead:\n",
    "\n",
    "```\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "pip install git+https://github.com/boudinfl/pke.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/site-packages/faiss-1.7.4-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: spacy in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (65.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib64/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "/scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/site-packages/faiss-1.7.4-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m124.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/site-packages/faiss-1.7.4-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/boudinfl/pke.git\n",
      "  Cloning https://github.com/boudinfl/pke.git to /tmp/attiehjo/4105511/pip-req-build-mpm0qiza\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/boudinfl/pke.git /tmp/attiehjo/4105511/pip-req-build-mpm0qiza\n",
      "  Resolved https://github.com/boudinfl/pke.git to commit 69871ffdb720b83df23684fea53ec8776fd87e63\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.11/site-packages (from pke==2.0.0) (3.9.1)\n",
      "Requirement already satisfied: networkx in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from pke==2.0.0) (3.4.2)\n",
      "Requirement already satisfied: numpy in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from pke==2.0.0) (1.26.4)\n",
      "Requirement already satisfied: scipy in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from pke==2.0.0) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from pke==2.0.0) (1.5.2)\n",
      "Requirement already satisfied: unidecode in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from pke==2.0.0) (1.3.8)\n",
      "Requirement already satisfied: future in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from pke==2.0.0) (1.0.0)\n",
      "Requirement already satisfied: joblib in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from pke==2.0.0) (1.4.2)\n",
      "Requirement already satisfied: spacy>=3.2.3 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from pke==2.0.0) (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (65.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from spacy>=3.2.3->pke==2.0.0) (3.5.0)\n",
      "Requirement already satisfied: click in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from nltk->pke==2.0.0) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from nltk->pke==2.0.0) (2024.11.6)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from scikit-learn->pke==2.0.0) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.2.3->pke==2.0.0) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.2.3->pke==2.0.0) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.2.3->pke==2.0.0) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /scratch/project_2007095/attieh/All_distillation_methods/env/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.2.3->pke==2.0.0) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib64/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Install spacy\n",
    "!{sys.executable} -m pip install spacy\n",
    "\n",
    "# Download the English SpaCy model\n",
    "!{sys.executable} -m spacy download en_core_web_sm\n",
    "\n",
    "# Install pke from GitHub\n",
    "!{sys.executable} -m pip install git+https://github.com/boudinfl/pke.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Environment Setup and Sample Documents**\n",
    "\n",
    "### **3.1. Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries import\n",
    "import os\n",
    "import pke\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# For computing embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# For clustering, similarity, and visualization\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Matplotlib & Seaborn configurations\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2. Define Sample Documents**\n",
    "\n",
    "The documents that we are defining deal with a broad range of topics:\n",
    "- Document 1: Technology\n",
    "- Document 2: Sports  \n",
    "- Document 3: Food  \n",
    "- Document 4: Technology\n",
    "- Document 5: Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Documents:\n",
      "\n",
      "Document 1: In today's digital era, technology drives innovation with state-of-the-art systems and breakthrough solutions. Emphasizing connectivity and creative problem-solving, this field transforms everyday life with a dynamic blend of precision and ingenuity.\n",
      "Document 2: In today's digital era, sports drive innovation with state-of-the-art training techniques and breakthrough strategies. Emphasizing connectivity and creative teamwork, this realm transforms athletic performance with a dynamic blend of precision and ingenuity.\n",
      "Document 3: Culinary arts celebrate a fusion of tradition and innovation, blending carefully selected ingredients into dynamic, flavor-rich experiences. Chefs artfully combine technique and creativity, crafting recipes that echo the spirit of progress found in other innovative fields.\n",
      "Document 4: Embodying the essence of digital innovation, technology reshapes the future with state-of-the-art systems and breakthrough methodologies. This modern realm thrives on connectivity and creative problem-solving, offering a dynamic platform for progress and technical excellence.\n",
      "Document 5: In the competitive world of sports, every match is a vibrant display of athletic prowess and strategic teamwork. The arena pulses with energy and precise coordination, as athletes challenge themselves to overcome obstacles and secure triumph through relentless determination.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "documents = [\n",
    "    \"In today's digital era, technology drives innovation with state-of-the-art systems and breakthrough solutions. Emphasizing connectivity and creative problem-solving, this field transforms everyday life with a dynamic blend of precision and ingenuity.\",\n",
    "    \n",
    "    \"In today's digital era, sports drive innovation with state-of-the-art training techniques and breakthrough strategies. Emphasizing connectivity and creative teamwork, this realm transforms athletic performance with a dynamic blend of precision and ingenuity.\",\n",
    "    \n",
    "    \"Culinary arts celebrate a fusion of tradition and innovation, blending carefully selected ingredients into dynamic, flavor-rich experiences. Chefs artfully combine technique and creativity, crafting recipes that echo the spirit of progress found in other innovative fields.\",\n",
    "    \n",
    "    \"Embodying the essence of digital innovation, technology reshapes the future with state-of-the-art systems and breakthrough methodologies. This modern realm thrives on connectivity and creative problem-solving, offering a dynamic platform for progress and technical excellence.\",\n",
    "    \n",
    "    \"In the competitive world of sports, every match is a vibrant display of athletic prowess and strategic teamwork. The arena pulses with energy and precise coordination, as athletes challenge themselves to overcome obstacles and secure triumph through relentless determination.\"\n",
    "]\n",
    "\n",
    "    \n",
    "themes = [\"Tech\", \"Sports\", \"Food\", \"Tech\", \"Sports\"]\n",
    "print(\"Sample Documents:\\n\")\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1}:\", doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Extracting Keyphrases with `pke.TopicRank`**\n",
    "\n",
    "**`pke`** uses a standard procedure:\n",
    "1. **Candidate Selection**: Identify candidate words or phrases.\n",
    "2. **Candidate Weighting**: Score the candidates (in `TopicRank`, this is done via a graph-based method).\n",
    "3. **N-best Selection**: Select the top `N` keyphrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how pke works. For this, we are going to use a raw text file called [wiki_gershwin.txt](wiki_gershwin.txt). We first import the module and initialize the keyphrase extraction model (here: TopicRank):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke\n",
    "\n",
    "extractor = pke.unsupervised.TopicRank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the content of the document, here document is expected to be in raw format (i.e. a simple text file). The document is automatically preprocessed and analyzed with SpaCy, using the language given in the parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The latest smartphone by Apple offers cutting-edge features and a sleek design.\"\n",
    "extractor.load_document(text, language='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keyphrase extraction consists of three steps:\n",
    "\n",
    "1. Candidate selection:  \n",
    "With TopicRank, the default candidates are sequences of nouns and adjectives (i.e. `(Noun|Adj)*`)\n",
    "\n",
    "2. Candidate weighting:  \n",
    "With TopicRank, this is done using a random walk algorithm.\n",
    "\n",
    "3. N-best candidate selection:  \n",
    "The 10 highest-scored candidates are selected. They are returned as (keyphrase, score) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted themes:\n",
      "=================\n",
      "0.30331   apple\n",
      "0.28524   cutting-edge features\n",
      "0.23409   latest smartphone\n",
      "0.17736   sleek design\n"
     ]
    }
   ],
   "source": [
    "extractor.candidate_selection()\n",
    "extractor.candidate_weighting()\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "\n",
    "print(\"Extracted themes:\")\n",
    "print(\"=================\")\n",
    "for keyphrase in keyphrases:\n",
    "    print(f'{keyphrase[1]:.5f}   {keyphrase[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try out different methods for extracting themes: supervised, unsupervised, graph. Compare the themes extracted. If your texts are in other languages than English, test the themes extraction for them and assess the quality. Is this something you might want to use for your final project?\n",
    "\n",
    "You can read more about the pke toolkit from their paper ([Boudin, 2016](https://aclanthology.org/C16-2015.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's put the code in a method!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topics(text, n_keyphrases=10, language='en'):\n",
    "    \"\"\"\n",
    "    Extract the top n_keyphrases from a text using pke's TopicRank.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The text from which to extract keyphrases.\n",
    "    n_keyphrases : int, default=10\n",
    "        Number of keyphrases to return.\n",
    "    language : str, default='en'\n",
    "        Language for SpaCy processing. Commonly 'en' for English.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    keyphrases : list of (phrase, score) tuples\n",
    "    \"\"\"\n",
    "    # Initialize the TopicRank extractor\n",
    "    extractor = pke.unsupervised.TopicRank()\n",
    "\n",
    "    # Load the text into the extractor\n",
    "    extractor.load_document(input=text, language=language)\n",
    "\n",
    "    # Candidate selection\n",
    "    extractor.candidate_selection()\n",
    "\n",
    "    # Candidate weighting (graph-based)\n",
    "    extractor.candidate_weighting()\n",
    "\n",
    "    # Retrieve the top n keyphrases\n",
    "    keyphrases = extractor.get_n_best(n=n_keyphrases)\n",
    "\n",
    "    return keyphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2. Apply the Function to Each Document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 - Extracted Themes:\n",
      "----------------------------------\n",
      "Score: 0.08775 | Keyphrase: \"state-of-the-art systems\"\n",
      "Score: 0.08509 | Keyphrase: \"innovation\"\n",
      "Score: 0.08321 | Keyphrase: \"technology\"\n",
      "Score: 0.08145 | Keyphrase: \"breakthrough solutions\"\n",
      "Score: 0.08093 | Keyphrase: \"creative problem-solving\"\n",
      "Score: 0.07981 | Keyphrase: \"digital era\"\n",
      "Score: 0.07801 | Keyphrase: \"connectivity\"\n",
      "Score: 0.07782 | Keyphrase: \"everyday life\"\n",
      "Score: 0.07654 | Keyphrase: \"field\"\n",
      "Score: 0.07547 | Keyphrase: \"dynamic blend\"\n",
      "\n",
      "\n",
      "Document 2 - Extracted Themes:\n",
      "----------------------------------\n",
      "Score: 0.09244 | Keyphrase: \"state-of-the-art training techniques\"\n",
      "Score: 0.09021 | Keyphrase: \"creative teamwork\"\n",
      "Score: 0.08949 | Keyphrase: \"sports drive innovation\"\n",
      "Score: 0.08753 | Keyphrase: \"athletic performance\"\n",
      "Score: 0.08702 | Keyphrase: \"breakthrough strategies\"\n",
      "Score: 0.08612 | Keyphrase: \"connectivity\"\n",
      "Score: 0.08582 | Keyphrase: \"realm\"\n",
      "Score: 0.08508 | Keyphrase: \"dynamic blend\"\n",
      "Score: 0.08341 | Keyphrase: \"precision\"\n",
      "Score: 0.08201 | Keyphrase: \"digital era\"\n",
      "\n",
      "\n",
      "Document 3 - Extracted Themes:\n",
      "----------------------------------\n",
      "Score: 0.09417 | Keyphrase: \"flavor-rich experiences\"\n",
      "Score: 0.08887 | Keyphrase: \"dynamic\"\n",
      "Score: 0.08494 | Keyphrase: \"chefs\"\n",
      "Score: 0.08387 | Keyphrase: \"technique\"\n",
      "Score: 0.08329 | Keyphrase: \"tradition\"\n",
      "Score: 0.08152 | Keyphrase: \"creativity\"\n",
      "Score: 0.07941 | Keyphrase: \"ingredients\"\n",
      "Score: 0.07652 | Keyphrase: \"innovation\"\n",
      "Score: 0.07520 | Keyphrase: \"fusion\"\n",
      "Score: 0.06942 | Keyphrase: \"recipes\"\n",
      "\n",
      "\n",
      "Document 4 - Extracted Themes:\n",
      "----------------------------------\n",
      "Score: 0.09732 | Keyphrase: \"state-of-the-art systems\"\n",
      "Score: 0.08966 | Keyphrase: \"breakthrough methodologies\"\n",
      "Score: 0.08867 | Keyphrase: \"digital innovation\"\n",
      "Score: 0.08845 | Keyphrase: \"future\"\n",
      "Score: 0.08577 | Keyphrase: \"technology\"\n",
      "Score: 0.08485 | Keyphrase: \"connectivity\"\n",
      "Score: 0.08365 | Keyphrase: \"creative problem-solving\"\n",
      "Score: 0.08321 | Keyphrase: \"progress\"\n",
      "Score: 0.08253 | Keyphrase: \"modern realm\"\n",
      "Score: 0.08109 | Keyphrase: \"dynamic platform\"\n",
      "\n",
      "\n",
      "Document 5 - Extracted Themes:\n",
      "----------------------------------\n",
      "Score: 0.14473 | Keyphrase: \"athletic prowess\"\n",
      "Score: 0.09078 | Keyphrase: \"energy\"\n",
      "Score: 0.08907 | Keyphrase: \"arena pulses\"\n",
      "Score: 0.08674 | Keyphrase: \"strategic teamwork\"\n",
      "Score: 0.08637 | Keyphrase: \"precise coordination\"\n",
      "Score: 0.08483 | Keyphrase: \"vibrant display\"\n",
      "Score: 0.07537 | Keyphrase: \"sports\"\n",
      "Score: 0.07433 | Keyphrase: \"match\"\n",
      "Score: 0.07278 | Keyphrase: \"triumph\"\n",
      "Score: 0.06618 | Keyphrase: \"obstacles\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_docs_topics = []\n",
    "\n",
    "for i, doc_text in enumerate(documents):\n",
    "    keyphrases = extract_topics(doc_text, n_keyphrases=10, language='en')\n",
    "    all_docs_topics.append(keyphrases)\n",
    "    \n",
    "    # Print a summary of extracted keyphrases\n",
    "    print(f\"Document {i+1} - Extracted Themes:\")\n",
    "    print(\"----------------------------------\")\n",
    "    for kp in keyphrases:\n",
    "        print(f\"Score: {kp[1]:.5f} | Keyphrase: \\\"{kp[0]}\\\"\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Extra Material to complement the reading material on Moodle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Embedding the Extracted Keyphrases**\n",
    "\n",
    "Instead of simply using **TF-IDF** on keyphrases (which may not appear as they are in the documents), we want to capture the **semantic meaning** of each keyphrase. To do this, we convert every extracted topic (keyphrase) into a **vector representation (embedding)**.\n",
    "\n",
    "By assuming that the overall meaning of a document is approximated by the **collective meaning** of its keyphrases, we can obtain a **single vector** for each document by **aggregating** (e.g., averaging) the embeddings of its keyphrases.\n",
    "\n",
    "**Steps**:  \n",
    "1. **Embed** each keyphrase using a [**SentenceTransformer**](https://www.sbert.net/) model (e.g., `all-MiniLM-L6-v2`).  \n",
    "2. **Weight** each embedding by the keyphrase’s **importance score** (from pke).  \n",
    "3. **Aggregate** these weighted embeddings (e.g., by taking their mean) to form a **single document vector**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = documents[0]\n",
    "topics = extract_topics(doc)\n",
    "print(\"Document : \", doc)\n",
    "print(\"Topics (topic, weight) : \", topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [t[1] for t in topics]\n",
    "phrases =[t[0] for t in topics]\n",
    "#Step 1 - Embed\n",
    "topics_embedding = model.encode(phrases)\n",
    "topics_embedding.shape, weights\n",
    "#Step 2 - Weight\n",
    "weighted_topics_embedding = np.array([ topics_embedding[i]*weights[i] for i in range(len(weights))])\n",
    "#Step 3 - Aggregate\n",
    "document_embedding = np.mean(weighted_topics_embedding, axis=0)\n",
    "weighted_topics_embedding.shape, document_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = []\n",
    "\n",
    "for doc_topics in all_docs_topics:\n",
    "    # doc_topics is a list of (keyphrase_string, score)\n",
    "    phrases = [t[0] for t in doc_topics]\n",
    "    scores = np.array([t[1] for t in doc_topics])\n",
    "\n",
    "    # Encode each keyphrase\n",
    "    phrase_embeddings = model.encode(phrases)\n",
    "\n",
    "    # Weight the embeddings by their scores\n",
    "    scores = scores.reshape(-1, 1)\n",
    "    weighted_embeddings = phrase_embeddings * scores\n",
    "\n",
    "    # Aggregate (mean) to get a single vector per document\n",
    "    doc_embedding = np.mean(weighted_embeddings, axis=0)\n",
    "\n",
    "    document_embeddings.append(doc_embedding)\n",
    "\n",
    "print(\"Number of document embeddings:\", len(document_embeddings))\n",
    "print(\"Dimension of each embedding:\", len(document_embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Clustering the Documents**\n",
    "\n",
    "We now have 5 **document vectors** derived from their keyphrases. Let's cluster them.\n",
    "\n",
    "#### What is Document Clustering?\n",
    "\n",
    "Document clustering (or text clustering) is an **unsupervised approach** (i.e., no labels are required for the documents we have). The goal is to **group documents into clusters (groups)** so that:\n",
    "\n",
    "- Documents in the **same cluster** are **similar** to each other.\n",
    "- Documents in **different clusters** are **dissimilar** from each other.\n",
    "\n",
    "#### What Do We Need for Document Clustering?\n",
    "\n",
    "We need two key components:\n",
    "\n",
    "1. **Document Representation:**\n",
    "   - We need to represent documents **numerically**.\n",
    "   - Example: Compute **TF-IDF** to represent documents based on the terms they contain.\n",
    "   - **Another method**: Extract themes and represent each document as a **vector**, where each dimension represents a theme.\n",
    "\n",
    "2. **A Similarity Measure:**\n",
    "   - We need to assess how similar two documents are in content.\n",
    "   - Common similarity measures include **cosine similarity** and **Euclidean distance**.\n",
    "\n",
    "We have the document representation, let's compute the similarity using **cosine similarity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(document_embeddings)\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the heatmap\n",
    "plt.imshow(similarities, cmap='viridis', vmin=0, vmax=1)\n",
    "\n",
    "\n",
    "plt.colorbar() # Add a colorbar (legend)\n",
    "\n",
    "# Create labels for each document (Doc0, Doc1, ...)\n",
    "num_docs = similarities.shape[0]\n",
    "fig_labels = [f\"Doc{i+1}\" for i in range(num_docs)]\n",
    "\n",
    "# Use the labels on the x-axis and y-axis\n",
    "plt.xticks(np.arange(num_docs), fig_labels)\n",
    "plt.yticks(np.arange(num_docs), fig_labels)\n",
    "\n",
    "\n",
    "plt.title(\"Document Similarities\") # Add a title\n",
    "\n",
    "# Overlay the numeric values on each cell\n",
    "for i in range(num_docs):\n",
    "    for j in range(num_docs):\n",
    "        # Format values with two decimals\n",
    "        plt.text(j, i, f\"{similarities[i, j]:.2f}\",\n",
    "                 ha='center', va='center', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.1. K-Means Clustering**\n",
    "\n",
    "We choose `k = 3` clusters for demonstration. Adjust as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 3  # number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "labels_kmeans = kmeans.fit_predict(document_embeddings)\n",
    "\n",
    "df_clusters_kmeans = pd.DataFrame({\n",
    "    'Document': [f\"Doc_{i+1}\" for i in range(len(documents))],\n",
    "    'Cluster': labels_kmeans,\n",
    "    'Themes': themes, \n",
    "})\n",
    "\n",
    "df_clusters_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2. Hierarchical Clustering**\n",
    "\n",
    "We will compute the linkage matrix for various linkage criteria. Then, we'll plot dendrograms to observe how documents group at different distance thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def perform_hierarchical_clustering(method_of_choice, num_clusters, documents, document_embeddings, themes):\n",
    "    \"\"\"\n",
    "    Performs hierarchical clustering on document embeddings using the specified linkage method.\n",
    "    \n",
    "    Parameters:\n",
    "        method_of_choice (str): Linkage method ('single', 'complete', 'average', 'ward').\n",
    "        num_clusters (int): The number of clusters to form.\n",
    "        documents (list of str): List of document texts.\n",
    "        document_embeddings (list or array-like): Document embeddings (each should be a 1D array).\n",
    "        themes (list): List of themes corresponding to each document.\n",
    "        tones (list): List of tones corresponding to each document.\n",
    "    \"\"\"\n",
    "    # Convert embeddings into a 2D NumPy array\n",
    "    X = np.vstack(document_embeddings)\n",
    "    \n",
    "    # Compute the linkage matrix for the chosen method\n",
    "    Z = linkage(X, method=method_of_choice)\n",
    "    \n",
    "    # Plot the dendrogram for the chosen method\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    dendrogram(Z, labels=[f\"Doc_{i+1}\" for i in range(len(documents))], leaf_rotation=90)\n",
    "    plt.title(f\"Hierarchical Clustering - {method_of_choice.capitalize()} Linkage\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate cluster labels using the 'maxclust' criterion\n",
    "    labels_hier = fcluster(Z, t=num_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Create a DataFrame to display clustering results\n",
    "    df_clusters_hier = pd.DataFrame({\n",
    "        'Document': [f\"Doc_{i+1}\" for i in range(len(documents))],\n",
    "        'Cluster': labels_hier,\n",
    "        'Themes': themes,\n",
    "        'Text': documents\n",
    "    })\n",
    "    \n",
    "    \n",
    "    # Group documents by cluster and print them\n",
    "    clusters = defaultdict(list)\n",
    "    for _, row in df_clusters_hier.iterrows():\n",
    "        clusters[row['Cluster']].append(row['Text'])\n",
    "    \n",
    "    for cluster, docs in clusters.items():\n",
    "        print(f\"\\n--- Cluster {cluster} ---\\n\")\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            # For readability, print only the first 500 characters of each document.\n",
    "            print(f\"[Doc {i}]: {doc[:500]}...\\n\")\n",
    "        print(\"=\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_hierarchical_clustering(\"single\", num_clusters, documents, document_embeddings, themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_hierarchical_clustering(\"complete\", num_clusters, documents, document_embeddings, themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_hierarchical_clustering(\"average\", num_clusters, documents, document_embeddings, themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_hierarchical_clustering(\"ward\", num_clusters, documents, document_embeddings, themes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Conclusion**\n",
    "\n",
    "In this notebook, we:\n",
    "1. **Installed** and imported `pke`, `spacy`, and `sentence-transformers`.\n",
    "2. Used **`pke.TopicRank`** to extract keyphrases (themes) from each sample document.\n",
    "3. **Embedded** these keyphrases into vectors and aggregated them per document.\n",
    "4. **Clustered** using K-Means and Hierarchical Clustering.\n",
    "\n",
    "### **Possible Next Steps**\n",
    "- **Try different pke algorithms** (e.g., `YAKE`, `TextRank`, `MultipartiteRank`).\n",
    "- **Tune** clustering parameters (e.g., different values of `k`, different linkage criteria).\n",
    "- Apply these methods to **larger or real-world datasets** and evaluate.\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
