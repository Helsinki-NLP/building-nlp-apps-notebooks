{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance-ranked search\n",
    "\n",
    "This notebook is an expansion of https://github.com/mathiascreutz/nlp-tutorials/blob/main/tutorials/relevance-ranked-search.ipynb.\n",
    "\n",
    "Let's return to the indexing of toy data, as we did in the tutorial on Boolean search. This new tutorial has also been inspired by course material by Filip Ginter in Turku.\n",
    "\n",
    "Our documents now look slightly different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"This is a silly silly silly example\",\n",
    "             \"A better example\",\n",
    "             \"Nothing to see here nor here nor here\",\n",
    "             \"This is a great example and a long example too\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can index them as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term-document matrix:\n",
      "\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 1 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "cv = CountVectorizer(lowercase=True, binary=True)\n",
    "binary_dense_matrix = cv.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"Term-document matrix:\\n\")\n",
    "print(binary_dense_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll remove the `binary=True` optional argument from the `CountVectorizer` constructor. The default value is `binary=False`. What change can we observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term-document matrix:\n",
      "\n",
      "[[0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [1 1 0 2]\n",
      " [0 0 0 1]\n",
      " [0 0 3 0]\n",
      " [1 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 2 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [3 0 0 0]\n",
      " [1 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(lowercase=True)\n",
    "dense_matrix = cv.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"Term-document matrix:\\n\")\n",
    "print(dense_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall what term each row in the matrix corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (row, term) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m()):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRow\u001b[39m\u001b[38;5;124m\"\u001b[39m, row, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis the vector for term:\u001b[39m\u001b[38;5;124m\"\u001b[39m, term)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names_out'"
     ]
    }
   ],
   "source": [
    "for (row, term) in enumerate(cv.get_feature_names_out()):\n",
    "    print(\"Row\", row, \"is the vector for term:\", term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we run a query on the term \"example\", we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: example\n",
      "[[1 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "t2i = cv.vocabulary_  # shorter notation: t2i = term-to-index\n",
    "print(\"Query: example\")\n",
    "print(dense_matrix[t2i[\"example\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of seeing *whether* a term occurs in a document, we now see *how many times* the term occurs in each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example occurs 1 time(s) in document: This is a silly silly silly example\n",
      "Example occurs 1 time(s) in document: A better example\n",
      "Example occurs 0 time(s) in document: Nothing to see here nor here nor here\n",
      "Example occurs 2 time(s) in document: This is a great example and a long example too\n"
     ]
    }
   ],
   "source": [
    "hits_list = np.array(dense_matrix[t2i[\"example\"]])[0]\n",
    "\n",
    "for i, nhits in enumerate(hits_list):\n",
    "    print(\"Example occurs\", nhits, \"time(s) in document:\", documents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number and sizes of the documents grow, we may think that the more times a search term occurs in a document, the more relevant the document is. So, if we search for \"example\" in our toy document collection, the fourth document is most relevant (2 hits), the first and second documents come next (1 hit each) and the third document is irrelevant (0 hits).\n",
    "\n",
    "If we have multiple search terms, we might think that the more times the search terms occur in total in the document, the more relevant the document is.\n",
    "\n",
    "Note that the bit-wise logical operators `AND (&)` and `OR (|)` will not work properly anymore when our matrix contains word counts. The same applies to `NOT (1 - x)`.\n",
    "\n",
    "Let's search for the most relevant document for the query *better example*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: better example\n",
      "Hits of better:         [[0 1 0 0]]\n",
      "Hits of example:        [[1 1 0 2]]\n",
      "Hits of better example: [[1 2 0 2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Query: better example\")\n",
    "print(\"Hits of better:        \", dense_matrix[t2i[\"better\"]])\n",
    "print(\"Hits of example:       \", dense_matrix[t2i[\"example\"]])\n",
    "print(\"Hits of better example:\", dense_matrix[t2i[\"better\"]] + dense_matrix[t2i[\"example\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just added the hits together. This means that we did not search for the phrase \"better example\", nor did we search for \"better\" AND \"example\". What we did search for was some kind of \"better\" OR \"example\", in which the sum of the number of occurrences of \"better\" and \"example\" in a document determines the relevance of the document.\n",
    "\n",
    "This means that the second document, which contains one occurrence each of \"better\" and \"example\" is as good a hit as the fourth document, which contains two occurrences of \"example\" and no occurrence of \"better\".\n",
    "\n",
    "Let's execute another query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: silly example\n",
      "Hits of silly:         [[3 0 0 0]]\n",
      "Hits of example:       [[1 1 0 2]]\n",
      "Hits of silly example: [[4 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Query: silly example\")\n",
    "print(\"Hits of silly:        \", dense_matrix[t2i[\"silly\"]])\n",
    "print(\"Hits of example:      \", dense_matrix[t2i[\"example\"]])\n",
    "print(\"Hits of silly example:\", dense_matrix[t2i[\"silly\"]] + dense_matrix[t2i[\"example\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and also rank (sort) the results by relevance. We leave out the document without a single hit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits: [4 1 0 2]\n",
      "List of tuples (nhits, doc_idx) where nhits > 0: [(4, 0), (1, 1), (2, 3)]\n",
      "Ranked (nhits, doc_idx) tuples: [(4, 0), (2, 3), (1, 1)]\n",
      "\n",
      "Matched the following documents, ranked highest relevance first:\n",
      "Score of 'silly example' is 4 in document: This is a silly silly silly example\n",
      "Score of 'silly example' is 2 in document: This is a great example and a long example too\n",
      "Score of 'silly example' is 1 in document: A better example\n"
     ]
    }
   ],
   "source": [
    "# We need the np.array(...)[0] code here to convert the matrix to an ordinary list:\n",
    "hits_list = np.array(dense_matrix[t2i[\"silly\"]] + dense_matrix[t2i[\"example\"]])[0]\n",
    "print(\"Hits:\", hits_list)\n",
    "\n",
    "nhits_and_doc_ids = [ (nhits, i) for i, nhits in enumerate(hits_list) if nhits > 0 ]\n",
    "print(\"List of tuples (nhits, doc_idx) where nhits > 0:\", nhits_and_doc_ids)\n",
    "\n",
    "ranked_nhits_and_doc_ids = sorted(nhits_and_doc_ids, reverse=True)\n",
    "print(\"Ranked (nhits, doc_idx) tuples:\", ranked_nhits_and_doc_ids)\n",
    "\n",
    "print(\"\\nMatched the following documents, ranked highest relevance first:\")\n",
    "for nhits, i in ranked_nhits_and_doc_ids:\n",
    "    print(\"Score of 'silly example' is\", nhits, \"in document:\", documents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf\n",
    "\n",
    "As we may guess, pure word counts are not a good indicator of relevance. Frequently occurring words are not usually very interesting from the point of view of information content.\n",
    "\n",
    "One approach to weight terms (words) by their relevance is to use *term frequency / inverse document frequency (tf-idf)* weighting. There is another [tutorial on tf-idf](https://github.com/mathiascreutz/nlp-tutorials/blob/main/tutorials/tf-idf-gutenberg.ipynb) that illustrates how this weighting works.\n",
    "\n",
    "As a matter of fact, the scikit-learn library makes it easy for us to compute the tf-idf scores of terms in a document collection. Instead of the class `CountVectorizer` we can use `TfidfVectorizer`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TfidfVectorizer can be used with many different parameter values. One option is to count ordinary term frequencies. In this setup the resulting matrix should produce the same values as the one produced by the CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer:\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 1. 0. 2.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 3. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 2. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [3. 0. 0. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "\n",
      "CountVectorizer:\n",
      "[[0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [1 1 0 2]\n",
      " [0 0 0 1]\n",
      " [0 0 3 0]\n",
      " [1 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 2 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [3 0 0 0]\n",
      " [1 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Parameters with which TfidfVectorizer does same thing as CountVectorizer\n",
    "tfv1 = TfidfVectorizer(lowercase=True, sublinear_tf=False, use_idf=False, norm=None)\n",
    "tf_matrix1 = tfv1.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"TfidfVectorizer:\")\n",
    "print(tf_matrix1)\n",
    "\n",
    "print(\"\\nCountVectorizer:\")\n",
    "print(dense_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are the same, except that the TfidfVectorizer produces floating-point values, whereas the CountVectorizer produces integer values.\n",
    "\n",
    "Some useful parameters for the TfidfVectorizer are `sublinear_tf`, `use_idf` and `norm`.\n",
    "\n",
    "`sublinear_tf=True` uses logarithmic word frequencies instead of linear ones. That is, if a term occurs 20 times, it is not 20 times more important than a term that occurs once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer (logarithmic term frequencies):\n",
      "[[0.         0.         0.         1.        ]\n",
      " [0.         1.         0.         0.        ]\n",
      " [1.         1.         0.         1.69314718]\n",
      " [0.         0.         0.         1.        ]\n",
      " [0.         0.         2.09861229 0.        ]\n",
      " [1.         0.         0.         1.        ]\n",
      " [0.         0.         0.         1.        ]\n",
      " [0.         0.         1.69314718 0.        ]\n",
      " [0.         0.         1.         0.        ]\n",
      " [0.         0.         1.         0.        ]\n",
      " [2.09861229 0.         0.         0.        ]\n",
      " [1.         0.         0.         1.        ]\n",
      " [0.         0.         1.         0.        ]\n",
      " [0.         0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfv2 = TfidfVectorizer(lowercase=True, sublinear_tf=True, use_idf=False, norm=None)\n",
    "tf_matrix2 = tfv2.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"TfidfVectorizer (logarithmic term frequencies):\")\n",
    "print(tf_matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`use_idf=True` factors in the inverse document frequencies. The more documents a term occurs in, the less relevant the term is, in general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer (logarithmic term frequencies and inverse document frequencies):\n",
      "[[0.         0.         0.         1.91629073]\n",
      " [0.         1.91629073 0.         0.        ]\n",
      " [1.22314355 1.22314355 0.         2.07096206]\n",
      " [0.         0.         0.         1.91629073]\n",
      " [0.         0.         4.02155128 0.        ]\n",
      " [1.51082562 0.         0.         1.51082562]\n",
      " [0.         0.         0.         1.91629073]\n",
      " [0.         0.         3.24456225 0.        ]\n",
      " [0.         0.         1.91629073 0.        ]\n",
      " [0.         0.         1.91629073 0.        ]\n",
      " [4.02155128 0.         0.         0.        ]\n",
      " [1.51082562 0.         0.         1.51082562]\n",
      " [0.         0.         1.91629073 0.        ]\n",
      " [0.         0.         0.         1.91629073]]\n"
     ]
    }
   ],
   "source": [
    "tfv3 = TfidfVectorizer(lowercase=True, sublinear_tf=True, use_idf=True, norm=None)\n",
    "tf_matrix3 = tfv3.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"TfidfVectorizer (logarithmic term frequencies and inverse document frequencies):\")\n",
    "print(tf_matrix3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If additionally, we use the L2 norm `norm=\"l2\"` we normalize all document vectors (columns) to have a (Euclidian) length of one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer (logarithmic term frequencies and inverse document frequencies, normalized document vectors):\n",
      "[[0.         0.         0.         0.39494151]\n",
      " [0.         0.84292635 0.         0.        ]\n",
      " [0.25939836 0.53802897 0.         0.42681878]\n",
      " [0.         0.         0.         0.39494151]\n",
      " [0.         0.         0.65482842 0.        ]\n",
      " [0.32040859 0.         0.         0.31137642]\n",
      " [0.         0.         0.         0.39494151]\n",
      " [0.         0.         0.52831145 0.        ]\n",
      " [0.         0.         0.31202925 0.        ]\n",
      " [0.         0.         0.31202925 0.        ]\n",
      " [0.85287113 0.         0.         0.        ]\n",
      " [0.32040859 0.         0.         0.31137642]\n",
      " [0.         0.         0.31202925 0.        ]\n",
      " [0.         0.         0.         0.39494151]]\n"
     ]
    }
   ],
   "source": [
    "tfv4 = TfidfVectorizer(lowercase=True, sublinear_tf=True, use_idf=True, norm=\"l2\")\n",
    "tf_matrix4 = tfv4.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"TfidfVectorizer (logarithmic term frequencies and inverse document frequencies, normalized document vectors):\")\n",
    "print(tf_matrix4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can search the index in the same way as above, even if we use tf-idf weighting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: silly example\n",
      "Hits of silly:         [[0.85287113 0.         0.         0.        ]]\n",
      "Hits of example:       [[0.25939836 0.53802897 0.         0.42681878]]\n",
      "Hits of silly example: [[1.11226949 0.53802897 0.         0.42681878]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Query: silly example\")\n",
    "print(\"Hits of silly:        \", tf_matrix4[t2i[\"silly\"]])\n",
    "print(\"Hits of example:      \", tf_matrix4[t2i[\"example\"]])\n",
    "print(\"Hits of silly example:\", tf_matrix4[t2i[\"silly\"]] + tf_matrix4[t2i[\"example\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we can rank the documents using the tf-idf scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits: [1.11226949 0.53802897 0.         0.42681878]\n",
      "List of tuples (hits, doc_idx) where hits > 0: [(1.1122694945914164, 0), (0.5380289691033573, 1), (0.42681878177600086, 3)]\n",
      "Ranked (hits, doc_idx) tuples: [(1.1122694945914164, 0), (0.5380289691033573, 1), (0.42681878177600086, 3)]\n",
      "\n",
      "Matched the following documents, ranked highest relevance first:\n",
      "Score of 'silly example' is 1.1123 in document: This is a silly silly silly example\n",
      "Score of 'silly example' is 0.5380 in document: A better example\n",
      "Score of 'silly example' is 0.4268 in document: This is a great example and a long example too\n"
     ]
    }
   ],
   "source": [
    "hits_list4 = np.array(tf_matrix4[t2i[\"silly\"]] + tf_matrix4[t2i[\"example\"]])[0]\n",
    "print(\"Hits:\", hits_list4)\n",
    "\n",
    "hits_and_doc_ids = [ (hits, i) for i, hits in enumerate(hits_list4) if hits > 0 ]\n",
    "print(\"List of tuples (hits, doc_idx) where hits > 0:\", hits_and_doc_ids)\n",
    "\n",
    "ranked_hits_and_doc_ids = sorted(hits_and_doc_ids, reverse=True)\n",
    "print(\"Ranked (hits, doc_idx) tuples:\", ranked_hits_and_doc_ids)\n",
    "\n",
    "print(\"\\nMatched the following documents, ranked highest relevance first:\")\n",
    "for hits, i in ranked_hits_and_doc_ids:\n",
    "    print(\"Score of 'silly example' is {:.4f} in document: {:s}\".format(hits, documents[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense that the document \"This is a silly silly silly example\" comes up on the top, but why does \"A better example\" now rank higher than \"This is a great example and a long example too\"? The former one contains only one occurrence of \"example\" whereas the latter one contains two. Can you figure out the reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "When we searched the index above, we scored the documents by summing together the tf-idf values of all the terms in the search query. A more sophisticated way is to transform the query itself into a document vector, in which we score each search term using tf-idf. We then compare the query vector to each document vector in the index. The more similar the query vector is to a document vector, the more relevant that document is for our search.\n",
    "\n",
    "Let us first create a vector of our query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.53802897 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.84292635 0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "query_vec4 = tfv4.transform([\"silly example\"]).todense()\n",
    "print(query_vec4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually a matrix with one row (document-term matrix). Since we have looked at term-document matrices above, let's transpose, to understand better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.53802897]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.84292635]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(query_vec4.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only two terms have non-zero values, and they are (not surprisingly) \"example\" and \"silly\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-idf weight of 'example' on row 2 is: [[0.53802897]]\n",
      "Tf-idf weight of 'silly' on row 10 is:  [[0.84292635]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tf-idf weight of 'example' on row\", t2i[\"example\"], \"is:\", query_vec4.T[t2i[\"example\"]])\n",
    "print(\"Tf-idf weight of 'silly' on row\", t2i[\"silly\"], \"is: \", query_vec4.T[t2i[\"silly\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you understand why the score of \"silly\" is higher than that of \"example\".\n",
    "\n",
    "To compare two vectors we use *cosine similarity*, which measures the cosine of the angle between the document vectors. If all vectors are guaranteed to be of length 1, which they are when we use the L2 norm, the cosine similarity reduces to the dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of 'silly example' is 0.8585 in document: This is a silly silly silly example\n",
      "The score of 'silly example' is 0.2895 in document: A better example\n",
      "The score of 'silly example' is 0.0000 in document: Nothing to see here nor here nor here\n",
      "The score of 'silly example' is 0.2296 in document: This is a great example and a long example too\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 4):\n",
    "    \n",
    "    # Go through each column (document vector) in the index \n",
    "    doc_vector = tf_matrix4[:, i]\n",
    "    \n",
    "    # Compute the dot product between the query vector and the document vector\n",
    "    # (Some extra stuff here to extract the number from the matrix data structure)\n",
    "    score = np.array(np.dot(query_vec4, doc_vector))[0][0]\n",
    "    \n",
    "    print(\"The score of 'silly example' is {:.4f} in document: {:s}\".format(score, documents[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the beauty with matrix and vector algebra, we don't actually need a loop, but we can do all calculations in one single dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The documents have the following cosine similarities to the query: [[0.85847138 0.28947517 0.         0.22964087]]\n"
     ]
    }
   ],
   "source": [
    "scores = np.dot(query_vec4, tf_matrix4)\n",
    "print(\"The documents have the following cosine similarities to the query:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to rank the matching documents, we can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of 'silly example' is 0.8585 in document: This is a silly silly silly example\n",
      "The score of 'silly example' is 0.2895 in document: A better example\n",
      "The score of 'silly example' is 0.2296 in document: This is a great example and a long example too\n"
     ]
    }
   ],
   "source": [
    "ranked_scores_and_doc_ids = \\\n",
    "    sorted([ (score, i) for i, score in enumerate(np.array(scores)[0]) if score > 0], reverse=True)\n",
    "\n",
    "for score, i in ranked_scores_and_doc_ids:\n",
    "    print(\"The score of 'silly example' is {:.4f} in document: {:s}\".format(score, documents[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling up to larger document collections with sparse matrices\n",
    "\n",
    "As we saw in the tutorial on Boolean search, any real-size data requires us to use sparse matrices. Let us go though how to use sparse matrices with tf-idf weighting.\n",
    "\n",
    "First we index the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse term-document matrix with tf-idf weights:\n",
      "  (0, 3)\t0.39494150730720773\n",
      "  (1, 1)\t0.8429263481500496\n",
      "  (2, 0)\t0.25939836420616813\n",
      "  (2, 1)\t0.5380289691033573\n",
      "  (2, 3)\t0.42681878177600086\n",
      "  (3, 3)\t0.39494150730720773\n",
      "  (4, 2)\t0.6548284187983\n",
      "  (5, 0)\t0.3204085857171691\n",
      "  (5, 3)\t0.31137642070883736\n",
      "  (6, 3)\t0.39494150730720773\n",
      "  (7, 2)\t0.5283114451514632\n",
      "  (8, 2)\t0.3120292501545813\n",
      "  (9, 2)\t0.3120292501545813\n",
      "  (10, 0)\t0.8528711303852483\n",
      "  (11, 0)\t0.3204085857171691\n",
      "  (11, 3)\t0.31137642070883736\n",
      "  (12, 2)\t0.3120292501545813\n",
      "  (13, 3)\t0.39494150730720773\n"
     ]
    }
   ],
   "source": [
    "tfv5 = TfidfVectorizer(lowercase=True, sublinear_tf=True, use_idf=True, norm=\"l2\")\n",
    "sparse_matrix = tfv5.fit_transform(documents).T.tocsr() # CSR: compressed sparse row format => order by terms\n",
    "\n",
    "print(\"Sparse term-document matrix with tf-idf weights:\")\n",
    "print(sparse_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we convert the query string to a sparse vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse one-row query matrix (horizontal vector):\n",
      "  (0, 2)\t0.5380289691033573\n",
      "  (0, 10)\t0.8429263481500496\n"
     ]
    }
   ],
   "source": [
    "# The query vector is a horizontal vector, so in order to sort by terms, we need to use CSC\n",
    "query_vec5 = tfv5.transform([\"silly example\"]).tocsc() # CSC: compressed sparse column format\n",
    "\n",
    "print(\"Sparse one-row query matrix (horizontal vector):\")\n",
    "print(query_vec5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute the cosine similarity (dot product). Since we are dealing with sparse matrices, any zero values are automatically left out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching documents and their scores:\n",
      "  (0, 0)\t0.858471381859184\n",
      "  (0, 1)\t0.2894751715944214\n",
      "  (0, 3)\t0.22964086915289256\n"
     ]
    }
   ],
   "source": [
    "hits = np.dot(query_vec5, sparse_matrix)\n",
    "\n",
    "print(\"Matching documents and their scores:\")\n",
    "print(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the document indexes like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matching documents are: [0 1 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"The matching documents are:\", hits.nonzero()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the tf-idf scores like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scores of the documents are: [0.85847138 0.28947517 0.22964087]\n"
     ]
    }
   ],
   "source": [
    "print(\"The scores of the documents are:\", np.array(hits[hits.nonzero()])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rank the documents by scores. It may be hard to see that this works, since the documents happen to be in the right order already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of 'silly example' is 0.8585 in document: This is a silly silly silly example\n",
      "The score of 'silly example' is 0.2895 in document: A better example\n",
      "The score of 'silly example' is 0.2296 in document: This is a great example and a long example too\n"
     ]
    }
   ],
   "source": [
    "ranked_scores_and_doc_ids = sorted(zip(np.array(hits[hits.nonzero()])[0], hits.nonzero()[1]), reverse=True)\n",
    "\n",
    "for score, i in ranked_scores_and_doc_ids:\n",
    "    print(\"The score of 'silly example' is {:.4f} in document: {:s}\".format(score, documents[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gutenberg corpus\n",
    "\n",
    "Let's finally index the Gutenberg corpus in NLTK, to get a feel for some real data.\n",
    "\n",
    "We start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.5/770.5 kB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.3 nltk-3.8.1 regex-2022.10.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/jovyan/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 books in the collection: ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download(['gutenberg'])\n",
    "\n",
    "booknames = nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "bookdata = list(nltk.corpus.gutenberg.raw(name) for name in booknames)\n",
    "\n",
    "print(\"There are\", len(bookdata), \"books in the collection:\", booknames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we index it using the TfidfVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terms in vocabulary: 42063\n"
     ]
    }
   ],
   "source": [
    "gv = TfidfVectorizer(lowercase=True, sublinear_tf=True, use_idf=True, norm=\"l2\")\n",
    "g_matrix = gv.fit_transform(bookdata).T.tocsr()\n",
    "\n",
    "print(\"Number of terms in vocabulary:\", len(gv.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function for searching this document collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_gutenberg(query_string):\n",
    "\n",
    "    # Vectorize query string\n",
    "    query_vec = gv.transform([ query_string ]).tocsc()\n",
    "\n",
    "    # Cosine similarity\n",
    "    hits = np.dot(query_vec, g_matrix)\n",
    "\n",
    "    # Rank hits\n",
    "    ranked_scores_and_doc_ids = \\\n",
    "        sorted(zip(np.array(hits[hits.nonzero()])[0], hits.nonzero()[1]),\n",
    "               reverse=True)\n",
    "    \n",
    "    # Output result\n",
    "    print(\"Your query '{:s}' matches the following documents:\".format(query_string))\n",
    "    for i, (score, doc_idx) in enumerate(ranked_scores_and_doc_ids):\n",
    "        print(\"Doc #{:d} (score: {:.4f}): {:s}\".format(i, score, booknames[doc_idx]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and run some searches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your query 'alice' matches the following documents:\n",
      "Doc #0 (score: 0.1046): carroll-alice.txt\n",
      "Doc #1 (score: 0.0106): edgeworth-parents.txt\n",
      "Doc #2 (score: 0.0092): chesterton-thursday.txt\n",
      "\n",
      "Your query 'alice entertained harriet' matches the following documents:\n",
      "Doc #0 (score: 0.0590): carroll-alice.txt\n",
      "Doc #1 (score: 0.0505): austen-emma.txt\n",
      "Doc #2 (score: 0.0092): edgeworth-parents.txt\n",
      "Doc #3 (score: 0.0052): chesterton-thursday.txt\n",
      "Doc #4 (score: 0.0045): austen-persuasion.txt\n",
      "Doc #5 (score: 0.0043): milton-paradise.txt\n",
      "Doc #6 (score: 0.0040): austen-sense.txt\n",
      "Doc #7 (score: 0.0039): chesterton-ball.txt\n",
      "Doc #8 (score: 0.0010): bible-kjv.txt\n",
      "\n",
      "Your query 'whale hunter' matches the following documents:\n",
      "Doc #0 (score: 0.0281): melville-moby_dick.txt\n",
      "Doc #1 (score: 0.0239): bryant-stories.txt\n",
      "Doc #2 (score: 0.0135): whitman-leaves.txt\n",
      "Doc #3 (score: 0.0112): chesterton-ball.txt\n",
      "Doc #4 (score: 0.0109): edgeworth-parents.txt\n",
      "Doc #5 (score: 0.0094): shakespeare-hamlet.txt\n",
      "Doc #6 (score: 0.0081): bible-kjv.txt\n",
      "Doc #7 (score: 0.0059): shakespeare-macbeth.txt\n",
      "Doc #8 (score: 0.0057): milton-paradise.txt\n",
      "\n",
      "Your query 'oh thy lord cometh' matches the following documents:\n",
      "Doc #0 (score: 0.0332): bible-kjv.txt\n",
      "Doc #1 (score: 0.0282): blake-poems.txt\n",
      "Doc #2 (score: 0.0254): shakespeare-hamlet.txt\n",
      "Doc #3 (score: 0.0231): shakespeare-macbeth.txt\n",
      "Doc #4 (score: 0.0202): shakespeare-caesar.txt\n",
      "Doc #5 (score: 0.0187): bryant-stories.txt\n",
      "Doc #6 (score: 0.0147): melville-moby_dick.txt\n",
      "Doc #7 (score: 0.0134): milton-paradise.txt\n",
      "Doc #8 (score: 0.0134): edgeworth-parents.txt\n",
      "Doc #9 (score: 0.0120): chesterton-thursday.txt\n",
      "Doc #10 (score: 0.0118): chesterton-ball.txt\n",
      "Doc #11 (score: 0.0108): chesterton-brown.txt\n",
      "Doc #12 (score: 0.0105): austen-emma.txt\n",
      "Doc #13 (score: 0.0104): austen-sense.txt\n",
      "Doc #14 (score: 0.0100): austen-persuasion.txt\n",
      "Doc #15 (score: 0.0090): carroll-alice.txt\n",
      "Doc #16 (score: 0.0085): whitman-leaves.txt\n",
      "Doc #17 (score: 0.0067): burgess-busterbrown.txt\n",
      "\n",
      "Your query 'which book should i read' matches the following documents:\n",
      "Doc #0 (score: 0.0487): carroll-alice.txt\n",
      "Doc #1 (score: 0.0417): blake-poems.txt\n",
      "Doc #2 (score: 0.0358): bryant-stories.txt\n",
      "Doc #3 (score: 0.0353): austen-persuasion.txt\n",
      "Doc #4 (score: 0.0335): burgess-busterbrown.txt\n",
      "Doc #5 (score: 0.0327): austen-sense.txt\n",
      "Doc #6 (score: 0.0316): austen-emma.txt\n",
      "Doc #7 (score: 0.0313): chesterton-brown.txt\n",
      "Doc #8 (score: 0.0296): chesterton-thursday.txt\n",
      "Doc #9 (score: 0.0288): edgeworth-parents.txt\n",
      "Doc #10 (score: 0.0284): chesterton-ball.txt\n",
      "Doc #11 (score: 0.0281): shakespeare-caesar.txt\n",
      "Doc #12 (score: 0.0246): shakespeare-macbeth.txt\n",
      "Doc #13 (score: 0.0246): milton-paradise.txt\n",
      "Doc #14 (score: 0.0224): shakespeare-hamlet.txt\n",
      "Doc #15 (score: 0.0211): whitman-leaves.txt\n",
      "Doc #16 (score: 0.0200): melville-moby_dick.txt\n",
      "Doc #17 (score: 0.0198): bible-kjv.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_gutenberg(\"alice\")\n",
    "search_gutenberg(\"alice entertained harriet\")\n",
    "search_gutenberg(\"whale hunter\")\n",
    "search_gutenberg(\"oh thy lord cometh\")\n",
    "search_gutenberg(\"which book should i read\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different ways term-document scores can be computed. In some approaches the query vector is not calculated in the same way as the document vectors. For instance, the idf factor may be used for query vectors, but left out from the document vectors. If you are interested, you can compare some different approaches on your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural/Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned before, tf-idf is a straight-forward and explainable way of getting vectors for each document and query. We can also do that with more sophisticated approaches such as using a pre-trained model to obtain dense vectors.\n",
    "\n",
    "We will use the model [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2), a compute-efficient sentence encoder. It is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures the semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Downloading pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting numpy>=1.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading numpy-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading sentence_transformers-3.4.0-py3-none-any.whl (275 kB)\n",
      "Downloading huggingface_hub-0.28.0-py3-none-any.whl (464 kB)\n",
      "Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.48.1-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading numpy-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: mpmath, urllib3, tqdm, threadpoolctl, sympy, safetensors, regex, pyyaml, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, joblib, idna, fsspec, filelock, charset-normalizer, certifi, triton, scipy, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, scikit-learn, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.1.0 certifi-2024.12.14 charset-normalizer-3.4.1 filelock-3.17.0 fsspec-2024.12.0 huggingface-hub-0.28.0 idna-3.10 jinja2-3.1.5 joblib-1.4.2 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.2 scikit-learn-1.6.1 scipy-1.15.1 sentence-transformers-3.4.0 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.21.0 torch-2.5.1 tqdm-4.67.1 transformers-4.48.1 triton-3.1.0 urllib3-2.3.0\n"
     ]
    }
   ],
   "source": [
    "# We install the required libraries\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/degibert/Documents/0_Work/Courses/building-nlp-apps-notebooks/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# We use a pretrained model from https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Small but effective model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 384)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 7.11015165e-02,  3.71286459e-02,  3.19642723e-02, -1.09879682e-02,\n",
       "        2.63225827e-02, -1.67778153e-02, -7.40993172e-02,  1.36819319e-03,\n",
       "        1.01486556e-02, -2.40272116e-02,  1.38697876e-02, -4.65226322e-02,\n",
       "        2.55219303e-02, -1.01622239e-01,  1.59779098e-04, -6.43442497e-02,\n",
       "        5.10249869e-04, -2.40349490e-02,  1.58969350e-02, -6.54078647e-02,\n",
       "        5.26457392e-02, -1.10656351e-01,  2.65473500e-02,  1.28463060e-02,\n",
       "       -9.26533118e-02, -8.47563520e-03, -7.23989159e-02,  2.67194901e-02,\n",
       "       -1.18807601e-02, -4.85659353e-02,  6.14764802e-02, -1.46089867e-02,\n",
       "       -8.23225379e-02,  6.90830573e-02, -1.60459038e-02,  3.92199233e-02,\n",
       "        5.07549457e-02, -7.05964565e-02, -1.49870068e-02, -1.70641057e-02,\n",
       "        1.61205996e-02, -2.17727330e-02, -2.79585179e-02,  1.10342484e-02,\n",
       "       -3.07924841e-02, -2.76145842e-02, -1.82458125e-02,  7.33444048e-03,\n",
       "        7.96843786e-03, -4.91103232e-02,  1.11571319e-01,  6.56414032e-02,\n",
       "        1.18013667e-02, -5.91971278e-02,  2.45729052e-02,  8.69821310e-02,\n",
       "        9.15037915e-02, -6.66655898e-02,  4.07437198e-02,  1.05212234e-01,\n",
       "        6.41662031e-02, -1.12434076e-02, -5.53918071e-02,  1.65754240e-02,\n",
       "        3.37992460e-02,  1.82338934e-02,  2.21248809e-02, -5.87714016e-02,\n",
       "       -4.43416797e-02, -1.34849653e-03, -6.40281290e-03, -1.57850105e-02,\n",
       "        4.18528020e-02,  1.70687530e-02,  3.41317393e-02,  2.30208375e-02,\n",
       "       -6.27132729e-02, -7.74422009e-03, -1.25257457e-02,  8.59236568e-02,\n",
       "       -4.93744295e-03, -5.37028760e-02,  3.12889405e-02, -3.70503752e-03,\n",
       "        6.06504865e-02,  1.76040735e-02,  3.09574679e-02,  6.11614175e-02,\n",
       "        3.47132497e-02,  7.71325408e-03, -1.26134548e-02, -2.07825415e-02,\n",
       "       -1.20306097e-01,  1.14002295e-01, -2.49900576e-02,  1.90779604e-02,\n",
       "       -6.71311244e-02, -2.96298676e-04, -6.17891215e-02, -2.58618183e-02,\n",
       "       -4.84941192e-02,  1.10580765e-01,  5.45172654e-02,  1.76274553e-02,\n",
       "       -6.56853169e-02, -3.37305218e-02,  1.05187192e-03,  5.63309975e-02,\n",
       "       -6.10871203e-02, -7.04088733e-02, -5.68199456e-02, -1.17997721e-01,\n",
       "        5.65295294e-02, -4.27927710e-02, -1.54103683e-02,  5.12362458e-02,\n",
       "       -1.41791096e-02, -1.08733475e-01, -4.59189005e-02, -3.40837538e-02,\n",
       "        6.17469735e-02,  9.53132752e-03,  2.69218702e-02,  6.82291910e-02,\n",
       "       -4.79592346e-02, -6.44571111e-02,  1.36784161e-03, -5.23395692e-33,\n",
       "        1.67173352e-02,  7.78665692e-02,  6.69688545e-03,  1.50767434e-03,\n",
       "        6.85869381e-02,  2.44305991e-02, -5.93625493e-02,  1.26022756e-01,\n",
       "        1.15706194e-02,  1.13740070e-02,  5.07667800e-03, -2.25285906e-03,\n",
       "       -2.43830029e-02,  4.73380871e-02,  3.76261845e-02, -9.07608122e-02,\n",
       "        9.02729556e-02,  2.51992904e-02, -9.21345688e-03,  1.92287900e-02,\n",
       "        6.82239830e-02, -3.11670620e-02,  2.22828332e-02, -8.13498069e-03,\n",
       "        3.97348171e-03,  3.10922079e-02, -6.61959276e-02,  3.89264487e-02,\n",
       "       -1.01283938e-02,  1.02735171e-02, -4.17610146e-02, -5.67070507e-02,\n",
       "        1.38541171e-02,  1.94929205e-02,  3.02757905e-03, -1.34935544e-03,\n",
       "        2.26121582e-02,  1.69826932e-02,  3.40702720e-02, -2.79008728e-02,\n",
       "        7.25445375e-02,  1.48311898e-03,  3.15095321e-03, -3.52307670e-02,\n",
       "       -2.39638835e-02,  7.87675977e-02,  3.83077934e-02, -1.01339981e-01,\n",
       "        1.34785250e-01, -7.75138736e-02,  3.19700837e-02, -4.28134063e-03,\n",
       "       -9.37673151e-02, -4.60429490e-02,  6.93942308e-02,  4.40119952e-02,\n",
       "       -8.02135281e-03,  3.01394262e-03,  7.26588145e-02, -2.78408602e-02,\n",
       "       -4.21374142e-02,  6.82958663e-02, -6.23116829e-02,  5.04880287e-02,\n",
       "        4.88150045e-02,  5.74617740e-03, -1.15124360e-02,  9.57938435e-04,\n",
       "       -6.15520552e-02, -2.33306736e-02, -5.17692976e-02,  1.60856023e-02,\n",
       "        1.02443136e-01, -2.53010876e-02,  1.96256470e-02,  8.43737572e-02,\n",
       "       -1.29141107e-01,  6.27140552e-02, -2.09867191e-02,  8.95372126e-03,\n",
       "       -2.16463450e-02,  2.65081152e-02,  1.33008622e-02,  5.92289791e-02,\n",
       "        7.28254765e-02, -4.89410721e-02, -1.18585173e-02,  9.53430310e-02,\n",
       "        7.86539260e-03,  9.81234852e-03, -5.76994680e-02, -1.02155335e-01,\n",
       "        2.94688158e-02, -2.10906677e-02, -1.72603894e-02,  1.54853307e-33,\n",
       "       -1.00717610e-02, -4.62132432e-02, -8.21128208e-03, -6.96331933e-02,\n",
       "        3.78043428e-02,  1.25418589e-01,  3.53237167e-02,  4.38123718e-02,\n",
       "        7.53421802e-03,  6.87101707e-02, -5.27454987e-02, -2.44977721e-03,\n",
       "        6.25343621e-02, -7.51251504e-02,  6.71974644e-02, -1.98664702e-03,\n",
       "        9.20675287e-04, -5.17435484e-02, -2.30415929e-02,  5.26910126e-02,\n",
       "       -6.44004904e-03, -1.37773249e-02, -3.24247293e-02,  5.56974672e-02,\n",
       "       -3.58206183e-02,  5.94308712e-02,  3.32205333e-02, -1.23355187e-01,\n",
       "        1.43982405e-02, -4.35930565e-02, -1.29520133e-01, -3.18313986e-02,\n",
       "        1.64369831e-03,  1.16412506e-01,  2.97234524e-02,  4.72835079e-02,\n",
       "        3.95374373e-02, -1.45095270e-02,  2.61703581e-02, -5.23000993e-02,\n",
       "       -3.30881588e-02,  3.98852415e-02, -1.23386283e-03, -6.34643361e-02,\n",
       "        5.87021522e-02,  1.76758436e-03, -6.25791550e-02, -2.23473534e-02,\n",
       "       -2.78999973e-02,  3.80100794e-02, -3.72099318e-02,  4.82462384e-02,\n",
       "       -1.03684261e-01, -6.32832944e-02, -2.70203967e-02,  2.69504637e-02,\n",
       "       -1.64929740e-02, -3.36466320e-02, -1.39525766e-02, -6.78131403e-03,\n",
       "       -1.25630794e-03,  5.55638447e-02, -3.79034989e-02,  4.81728651e-02,\n",
       "       -7.95510635e-02, -7.85289928e-02, -2.76749544e-02,  4.47994992e-02,\n",
       "        4.52209525e-02,  2.74582095e-02,  9.21858707e-04,  2.23474596e-02,\n",
       "       -4.01283316e-02,  2.91145928e-02, -7.18652504e-03, -1.13841696e-02,\n",
       "        5.40840290e-02,  2.04568692e-02,  3.66223380e-02,  3.58141288e-02,\n",
       "        2.44398434e-02,  2.71257944e-02, -3.72119844e-02, -6.02356577e-03,\n",
       "        1.23172011e-02, -3.38759311e-02,  1.55438706e-02, -1.71688404e-02,\n",
       "        1.06045835e-01, -1.05918422e-01,  3.40160131e-02, -2.05248520e-02,\n",
       "       -6.30502999e-02, -3.35574187e-02,  9.24764350e-02, -1.68367293e-08,\n",
       "       -1.08555099e-03,  6.64739460e-02,  2.32149046e-02, -5.58846779e-02,\n",
       "        1.35723094e-03, -1.87899530e-01,  7.09361956e-02,  4.80194390e-02,\n",
       "        5.78289386e-04, -9.79773626e-02,  3.27533409e-02, -2.33377721e-02,\n",
       "        3.12580820e-03, -1.61886849e-02, -9.76612791e-03, -2.46715527e-02,\n",
       "       -1.67441200e-02, -7.44395331e-02,  4.59325984e-02,  9.88942012e-02,\n",
       "       -3.21888067e-02, -1.12187685e-02,  4.28257547e-02, -3.54438573e-02,\n",
       "       -3.85993943e-02, -3.81525792e-02, -5.69159584e-03, -9.41332057e-02,\n",
       "       -4.50549610e-02,  5.62592298e-02,  1.33810695e-02, -4.23439890e-02,\n",
       "       -6.69110864e-02, -1.56239634e-02,  1.34931589e-02,  3.54464427e-02,\n",
       "        8.04511085e-02, -1.78613756e-02, -2.87162699e-02,  1.25982135e-03,\n",
       "        8.57019648e-02, -8.27651620e-02,  1.25032078e-04, -2.10620044e-03,\n",
       "        7.36563578e-02,  3.57916653e-02,  3.44043076e-02, -1.01526715e-01,\n",
       "        8.13031793e-02, -4.37361130e-04, -4.95448336e-02,  3.98392901e-02,\n",
       "       -2.61441153e-02, -4.61403131e-02, -3.99273746e-02, -4.39970866e-02,\n",
       "       -5.27984686e-02, -4.74736802e-02,  5.03543504e-02, -1.44452825e-02,\n",
       "        4.82310914e-02,  1.92936584e-02,  6.47123307e-02,  5.45099974e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We declare and encode our documents\n",
    "documents = [\"The Eiffel Tower is in Paris.\",\n",
    "            \"Mount Everest is the highest mountain.\",\n",
    "            \"Python is a popular programming language.\",\n",
    "            \"Paris is the capital of France.\"]\n",
    "\n",
    "doc_embeddings = model.encode(documents)\n",
    "print(doc_embeddings.shape)\n",
    "doc_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.40369695e-02,  7.31283575e-02, -1.12389792e-02,  4.75877486e-02,\n",
       "       -2.30635460e-02,  7.48442777e-04, -5.47064319e-02, -5.92756202e-04,\n",
       "       -6.90620020e-03, -4.42262553e-02,  3.36680636e-02, -7.00417608e-02,\n",
       "        4.30453382e-02, -9.29358676e-02,  9.47597530e-03, -2.49422397e-02,\n",
       "       -3.48448334e-03, -2.46436000e-02,  2.02209074e-02, -9.00812745e-02,\n",
       "        4.24851030e-02, -1.00372948e-01,  2.38506626e-02, -1.01565002e-02,\n",
       "       -3.57761681e-02,  1.01311198e-02, -7.76870772e-02,  5.92276268e-02,\n",
       "       -4.96612070e-03, -9.36788991e-02,  2.31905226e-02, -3.17058451e-02,\n",
       "       -5.74658252e-02,  4.83381934e-02,  1.54677569e-03,  7.13536292e-02,\n",
       "        6.79791942e-02, -4.21896838e-02,  3.38828005e-02, -2.25300696e-02,\n",
       "       -1.14800623e-02,  4.04713955e-03,  8.37309007e-03,  1.69057958e-02,\n",
       "       -3.76223251e-02, -2.61719935e-02, -2.80493032e-02, -1.40449535e-02,\n",
       "        4.24632952e-02, -4.29379717e-02,  9.51183438e-02,  3.02840807e-02,\n",
       "        2.70507876e-02, -4.64266501e-02,  8.05574730e-02,  8.33756179e-02,\n",
       "        6.85773119e-02, -1.03991807e-01,  6.58987239e-02,  6.25445321e-02,\n",
       "        1.11689523e-01,  1.35445800e-02, -8.24796408e-02,  2.77593099e-02,\n",
       "        1.06268525e-02, -3.64417061e-02, -4.14572097e-03, -9.72998589e-02,\n",
       "        2.39556143e-03, -8.26963410e-02,  1.85922235e-02, -4.78739776e-02,\n",
       "        4.23938222e-02, -2.09835116e-02, -2.95818062e-03,  9.62795690e-03,\n",
       "       -2.46488210e-02, -8.47271271e-03, -3.11173126e-02,  7.60357380e-02,\n",
       "       -4.75892201e-02, -2.99044531e-02,  1.99600495e-02,  4.76046987e-02,\n",
       "        3.58247310e-02,  3.45768295e-02,  1.71253793e-02,  5.73702268e-02,\n",
       "        2.33552009e-02,  3.06728552e-03,  1.63544919e-02, -5.78038357e-02,\n",
       "       -1.32754534e-01,  1.11840248e-01, -1.11272028e-02,  1.72328111e-02,\n",
       "       -7.53758550e-02, -6.54507568e-03, -6.46375418e-02,  1.20926965e-02,\n",
       "       -4.21923166e-03,  1.22528128e-01,  5.29022552e-02,  4.93289493e-02,\n",
       "       -6.13234602e-02, -5.96063808e-02,  1.77686214e-02,  1.03548601e-01,\n",
       "       -5.61035536e-02, -7.86177292e-02, -4.01962474e-02, -1.31999075e-01,\n",
       "        3.51872668e-02, -1.05071375e-02, -1.90515071e-02,  4.35550772e-02,\n",
       "        3.34953927e-02, -8.70144516e-02, -7.40400851e-02, -3.48941982e-02,\n",
       "        9.44967419e-02,  1.27210887e-02,  2.31620204e-03,  6.02806024e-02,\n",
       "       -5.32170944e-02, -8.06033239e-02, -3.18410210e-02, -5.65960930e-33,\n",
       "        2.25456301e-02,  1.76502503e-02,  1.92493740e-02,  3.64088751e-02,\n",
       "        1.04980186e-01,  2.08624694e-02, -5.75895496e-02,  1.02738626e-01,\n",
       "        6.50650542e-03,  2.41759736e-02,  1.73677281e-02,  6.86092395e-03,\n",
       "       -1.97029132e-02, -7.91737903e-03,  4.33642901e-02, -9.12988558e-02,\n",
       "        1.55890100e-02, -1.72901787e-02, -3.55111547e-02,  1.18958391e-02,\n",
       "        6.47969767e-02, -1.28348917e-02,  1.20875444e-02, -2.19539497e-02,\n",
       "       -2.96958932e-03,  4.33087274e-02, -4.99064960e-02,  2.26532537e-02,\n",
       "        1.42377038e-02,  5.14196940e-02, -2.74719894e-02, -8.89500678e-02,\n",
       "       -2.55136602e-02,  1.74951032e-02,  2.27541639e-03,  8.48761294e-04,\n",
       "        1.93540882e-02, -2.34976318e-02,  6.01587212e-03, -3.74354497e-02,\n",
       "        6.34191111e-02,  2.81459070e-03, -7.08645908e-03,  2.96633877e-02,\n",
       "       -2.56527718e-02,  4.39373925e-02,  3.57979536e-02, -8.83249864e-02,\n",
       "        1.20122373e-01, -9.32285190e-02,  2.94167418e-02,  4.08232808e-02,\n",
       "       -8.03329945e-02, -4.08546291e-02,  4.28594165e-02,  3.21584977e-02,\n",
       "        2.15029195e-02,  1.45701552e-02,  7.72945136e-02,  1.53427944e-02,\n",
       "       -1.31087052e-02,  8.81042108e-02, -5.07983714e-02,  3.27047110e-02,\n",
       "        5.42671382e-02, -3.23523954e-02, -2.09300108e-02, -8.01018532e-03,\n",
       "       -3.84192169e-02, -2.09231563e-02, -2.79488857e-03,  1.92176772e-03,\n",
       "        9.31563303e-02, -1.02396123e-02, -1.35345841e-02,  3.80641297e-02,\n",
       "       -1.58915609e-01,  6.22821786e-02, -1.37755126e-02,  1.23357335e-02,\n",
       "       -4.05024253e-02,  5.49621396e-02,  5.25090983e-03,  6.91602081e-02,\n",
       "        6.31061569e-02, -8.11068565e-02, -7.93191232e-03,  6.06213920e-02,\n",
       "       -1.21507328e-03, -8.92618392e-03, -9.19122770e-02, -6.95750788e-02,\n",
       "        6.01034611e-02, -3.82329300e-02, -1.43930558e-02,  2.53028943e-33,\n",
       "       -1.31917233e-02, -7.09545836e-02, -5.12998784e-03, -9.27608088e-02,\n",
       "       -1.55107081e-02,  1.12914324e-01,  4.27104980e-02,  7.50811473e-02,\n",
       "       -5.36576938e-03,  9.75604281e-02, -4.17749807e-02,  7.43642524e-02,\n",
       "        7.40608722e-02, -3.77078727e-02,  1.12078503e-01,  1.54772280e-02,\n",
       "        6.59370273e-02, -1.76073592e-02, -6.55842647e-02,  2.60863849e-03,\n",
       "        2.81241350e-03,  1.26061228e-03, -4.32369411e-02,  4.29088622e-02,\n",
       "       -1.86247062e-02,  9.07778665e-02,  1.65265892e-02, -1.33871600e-01,\n",
       "       -1.32140471e-02, -1.78775769e-02, -1.17160067e-01, -7.85901323e-02,\n",
       "       -3.47674228e-02,  1.00050241e-01, -5.67384250e-03,  4.87873293e-02,\n",
       "        1.94932353e-02, -1.73677467e-02,  2.50423364e-02, -7.16491230e-03,\n",
       "        2.67886762e-02,  3.21766734e-02, -7.80794118e-03,  7.28945248e-03,\n",
       "        4.86046374e-02, -1.26042729e-02, -5.10159843e-02,  1.86847020e-02,\n",
       "       -1.34912198e-02,  1.00348601e-02, -5.33922203e-02,  4.38865311e-02,\n",
       "       -5.78233786e-02, -3.96153443e-02,  5.74819418e-03,  2.08181832e-02,\n",
       "       -3.46498229e-02, -2.13369112e-02, -2.59771594e-03, -1.29482914e-02,\n",
       "       -1.06007711e-03,  6.22031726e-02, -4.28460315e-02,  5.48576117e-02,\n",
       "       -6.73155114e-02, -3.29085765e-03, -1.68142132e-02,  3.51300165e-02,\n",
       "       -4.18724632e-03, -1.49193071e-02, -1.20302243e-02,  4.81451079e-02,\n",
       "       -4.28630188e-02, -1.69662908e-02, -3.53940167e-02,  3.87273543e-02,\n",
       "        6.71631619e-02,  2.65028104e-02,  5.30614750e-03,  2.51894118e-03,\n",
       "        2.44446862e-02,  8.06778611e-04, -4.40311022e-02, -4.99829948e-02,\n",
       "        1.00666964e-02, -6.60885349e-02,  5.39278127e-02,  7.95772020e-03,\n",
       "        8.36342052e-02, -9.50894430e-02,  4.49526161e-02,  1.95277459e-03,\n",
       "       -5.15418574e-02, -3.81214246e-02,  6.93643168e-02, -1.53110840e-08,\n",
       "       -3.52364443e-02,  9.55702886e-02,  2.52845399e-02, -8.16377550e-02,\n",
       "        3.76496390e-02, -1.34412125e-01,  9.08416435e-02,  6.84858710e-02,\n",
       "       -8.80875625e-03, -6.46484643e-02,  7.57522210e-02, -3.51159945e-02,\n",
       "       -1.13892220e-02,  8.76254309e-03,  1.27743557e-02, -1.78560968e-02,\n",
       "       -4.60854433e-02, -7.31270760e-02,  3.89500856e-02,  4.90164869e-02,\n",
       "       -4.81237732e-02, -3.43485773e-02,  4.44296598e-02, -2.54709423e-02,\n",
       "       -7.05557540e-02,  4.13604872e-03,  2.65230294e-02, -8.89134333e-02,\n",
       "       -1.01136621e-02,  5.69429211e-02,  2.73686536e-02, -1.04134600e-03,\n",
       "       -6.80134892e-02, -7.70360138e-03, -2.45680828e-02,  2.13710200e-02,\n",
       "        5.49244583e-02, -2.69649010e-02, -4.38303724e-02, -1.91755239e-02,\n",
       "        5.07788025e-02, -7.81168193e-02, -2.36559776e-03,  2.68668402e-02,\n",
       "        7.04358071e-02,  8.62944946e-02, -2.68736086e-03, -4.39234935e-02,\n",
       "        5.27747683e-02, -4.37763408e-02, -6.97568208e-02,  2.40103453e-02,\n",
       "        3.17986459e-02, -2.32396759e-02, -3.11286021e-02, -5.82968108e-02,\n",
       "       -2.24314854e-02, -9.82889012e-02,  1.29002668e-02,  9.90376342e-03,\n",
       "        6.69930130e-02,  8.01650807e-03,  6.46506995e-02,  7.36175403e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We declare and encode our query\n",
    "query = \"Where is the Eiffel Tower?\"\n",
    "query_embedding = model.encode(query)\n",
    "query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your query 'Where is the Eiffel Tower?' matches the following documents:\n",
      "Doc #0 (score: 0.8585): The Eiffel Tower is in Paris.\n",
      "Doc #1 (score: 0.3081): Paris is the capital of France.\n",
      "Doc #2 (score: 0.2097): Mount Everest is the highest mountain.\n",
      "Doc #3 (score: -0.0083): Python is a popular programming language.\n"
     ]
    }
   ],
   "source": [
    "# We apply cosine similarity\n",
    "\n",
    "# Search for most similar document\n",
    "cosine_similarities = np.dot(query_embedding, doc_embeddings.T)\n",
    "\n",
    "# Rank hits (higher is better)\n",
    "ranked_doc_indices = np.argsort(cosine_similarities)[::-1]  # Sort descending\n",
    "\n",
    "# Output results\n",
    "print(f\"Your query '{query}' matches the following documents:\")\n",
    "for i, doc_idx in enumerate(ranked_doc_indices):\n",
    "    print(f\"Doc #{i} (score: {cosine_similarities[doc_idx]:.4f}): {documents[doc_idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
